#!/usr/bin/env python3
"""
GhostAI DLP SDK - Localhost Proxy Runner
Simple Flask proxy server for localhost testing
"""

import os
import sys
from ghostai.proxy_api.proxy import GhostAIProxy

def main():
    print("ğŸš€ GhostAI DLP Proxy - Localhost Runner")
    print("=" * 50)
    
    # Check if we should use mock mode
    use_mock = os.getenv('USE_MOCK_LLM', 'true').lower() == 'true'
    
    if use_mock:
        print("ğŸ¤– Using Mock LLM Server (no external API calls)")
        print("ğŸ“ Mock LLM: http://localhost:5005")
    else:
        print("ğŸŒ Using OpenAI API")
        # Use environment variable or set default
        if 'OPENAI_API_KEY' not in os.environ:
            os.environ['OPENAI_API_KEY'] = 'dummy-key-for-testing'
    
    # Initialize proxy
    proxy = GhostAIProxy(use_mock=use_mock)
    
    print("ğŸŒ Starting proxy server...")
    print("ğŸ“ API Endpoint: http://localhost:5004/v1/chat/completions")
    print("ğŸ” Health Check: http://localhost:5004/health")
    print("\nğŸ“ Test with curl:")
    print('curl -X POST http://localhost:5004/v1/chat/completions \\')
    print('  -H "Content-Type: application/json" \\')
    print('  -d \'{"messages":[{"role":"user","content":"My SSN is 123-45-6789"}]}\'')
    print("\nâ¹ï¸  Press Ctrl+C to stop")
    print("-" * 50)
    
    try:
        proxy.run(host='0.0.0.0', port=5004, debug=False)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Proxy stopped.")

if __name__ == "__main__":
    main()
